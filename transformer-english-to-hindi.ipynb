{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Input Embedding**","metadata":{}},{"cell_type":"code","source":"class InputEmbeddings(nn.Module):\n    \n    def __init__(self, d_model : int, vocab_size : int):\n        super().__init__() \n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n    def forward(self, x):\n        return self.embedding(x) * self(d_model)**-0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Positional Embedding**","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model : int, seq_len : int, dropout : float) -> None:\n        super().__init__()\n        self.d_model = d_model \n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        \n        #matrix creation of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) \n        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n        \n        self.register_buffer('pe', pe)\n        \n    def forward(self,x):\n        x = x + (self.pe[:, :x_shape[1], :]).requires_grad_(False)\n        return self.dropout(x)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Encoder Block**","metadata":{}},{"cell_type":"markdown","source":"# **Layer Normalization**","metadata":{}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    \n    def __init__(self, eps: float = 10** - 6) -> None:\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(1))\n        self.bias = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True)\n        std = x.std(dim = -1, keepdim = True)\n        return self.alpha * (x- mean) / (std + self.eps) + self.bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feed forward**","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    \n    def __init__(self,d_model : int,  d_ff: int, dropout : float) -> None:\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        \n    def forward(self, x):\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Multihead Attention**","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \n    def __init__(self, d_model : int, h : int, dropout : float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        self.h = h\n        assert d_model % h == 0\n        \n        self.d_k = d_model // h\n        \n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        \n        self.w_o = nn.Linear(d_model, d_model)\n    \n    @staticmethod\n    def selfattention(query, key, value, mask, dropout : nn.Dropout):\n        d_k = query.shape[-1]\n        attention_scores = (query @ key.transpose(-2, -1)) // math.sqrt(d_k)\n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, float('-inf'))\n        attention_scores = attention_scores.softmax(dim = -1) \n        \n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        \n        return (attention_scores @ value), attention_scores\n        \n    def forward(self, q,k,v,mask):\n        query = self.w_q(q) #batch, seq_len, d_model -> batch, seq_len, d_model\n        key = self.w_k(k)   #batch, seq_len, d_model -> batch, seq_len, d_model\n        value = self.w_v(v) #batch, seq_len, d_model -> batch, seq_len, d_model\n            \n            \n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # batch, seq_len, d_model -> batch, seq_len, h, d_k -> batch, h, seq_len, d_k \n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n        \n        x.self.attention_scores = MultiHeadAttention.selfattention(query, key, value, mask, self.dropout)\n        \n        # batch, h, seq_len, d_k -> batch, seq_len, h, d_k -> batch, seq_len, d_model\n        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n        \n        # batch, seq_len, d_model ->  batch, seq_len, d_model\n        return self.w_o(x)\n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Residual Connections**","metadata":{}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    \n    def __init__(self, features: int, dropout: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization(features)\n        \n        \n        def forward(self, x, sublayer):\n            #return x + self.norm(sublayer(self.dropout(x)))\n            return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Block Completion**","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    \n    def __init__(self,features: int, self_attention_block : MultiHeadAttention, feed_forward_block : FeedForward, dropout : float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([\n            ResidualConnection(dropout) for _ in range(2)\n        ])\n        \n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,src_mask))\n        x = self.feed_forward_block[1](x, self.feed_forward_block)\n        return x\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, layers : nn.ModuleList)->None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)    \n        return self.norm(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Decoder** ","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForward, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Decoder Block**","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self, layers : nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)\n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Linear Layer** \n**(Output Layer with Softmax Application)**","metadata":{}},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n    \n    def __init__(self, d_model : int, vocab_size: int) -> None:\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n        \n    def forward(self, x):\n        #batch, seq_len, d_model -> batch, seq_len, vocab_size\n        return torch.log_softmax(self.proj(x), dim = -1)\n    \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Transformer Architecture**","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, encoder : Encoder, decoder : Decoder, src_embed: InputEmbeddings, tgt_embed : InputEmbeddings, src_pos : PositionalEncoding, tgt_pos : PositionalEncoding, projection_layer : ProjectionLayer) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n    \n    def encode(self, src, src_mask):\n        src = self.src_embed(src)\n        src = self.src_pos(src)\n        return self.encoder(src, src_mask)\n    \n    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n        tgt = self.tgt_embed(tgt)\n        tgt = self.tgt_pos(tgt)\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n    \n    def project(self, x):\n        return self.projection_layer(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Combining all the functions**\n> Build Transformer function \n\nCode with this block can be used for any task that requires a transformer architecture, not just the language conversion task that i will do in my notebook. **Naming that i have used in the next block is specified to be used in my task, that is hindi to english translation, and that will have to be changed when using it for a specified task.** ","metadata":{}},{"cell_type":"code","source":"def build_transformer(src_vocab_size : int, tgt_vocab_size : int, src_seq_len : int, tgt_seq_len : int, d_model : int = 512, N : int = 6, h: int = 8, dropout : float = 0.1, d_ff : int = 2048):\n    src_embed = InputEmbeddings(d_model, src_vocab_size)\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n    \n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n    \n    # ENCODER BLOCK\n    \n    encoder_blocks = []\n    \n    \n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n     \n    # DECODER BLOCK\n    \n    decoder_blocks = []\n    \n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout)\n        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n        \n    encoder = Encoder(nn.ModuleList(encoder_block))\n    decoder = Decoder(nn.ModuleList(decoder_block))\n    \n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n    \n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n    \n    # Xavier Uniform\n    # Parameters are initialized by xavier uniform algorithm which makes the training faster by not initializing parameters randomly.\n\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n            \n    return transformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tokenizer** \n> **word level** ","metadata":{}},{"cell_type":"code","source":"pip install tokenizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom pathlib import Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency= 2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ntokenizer_folder_path = '/kaggle/working/tokenizer'\nmodel_folder_path = '/kaggle/working/weights'\n\nos.makedirs(tokenizer_folder_path, exist_ok=True)\nos.makedirs(model_folder_path, exist_ok=True)\n\nprint(f\"Folder '{tokenizer_folder_path}' created successfully!\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_config():\n    config = {\n        \"datasource\" : 'cfilt/iitb-english-hindi',\n        \"num_epochs\": 20,\n        \"lr\": 10**-4,\n        \"lang_src\": \"en\",  \n        \"lang_tgt\": \"hi\",\n        \"seq_len\": 2200,\n        \"batch_size\": 4,\n        \"d_model\": 512,\n        \"model_folder\": \"/kaggle/working/weights\",\n        \"model_basename\": \"/kaggle/working/weights/tmodel_\",\n        \"tokenizer_file\": \"/kaggle/working/tokenizer/tokenizer_{}.json\",\n        \"preload\": None,\n        \"experiment_name\": \"/kaggle/working/runs/tmodel\"\n    }\n    \n    return config\n\nconfig = get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\nfrom datasets import load_dataset\n\ndef get_ds(config):\n    \n    ds_train = load_dataset('cfilt/iitb-english-hindi', 'default', split='train')\n    \n    return ds_train\n\nds_train = get_ds(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder']\n    model_basename = config['model_basename']\n    model_filename = f\"{model_basename}{epoch}.pt\"\n    return str(Path('_') / model_folder / model_filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_weights_file_path(config, epoch: str):\n    model_folder = config ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_src = get_or_build_tokenizer(config, ds_train, config['lang_src'])\ntokenizer_tgt = get_or_build_tokenizer(config, ds_train, config['lang_tgt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_ds_size = int(0.9* len(ds_train))\nval_ds_size = len(ds_train) - train_ds_size\ntrain_ds, val_ds = random_split(ds_train, [train_ds_size, val_ds_size])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset**","metadata":{}},{"cell_type":"code","source":"def casual_mask(size):\n    mask = torch.triu(torch.ones(1,size,size, diagonal = 1).type(torch.int))\n    return mask == 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Any\n\nclass BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n        \n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.seq_len = seq_len\n        \n        self.sos_token = torch.tensor([tokenizer_src.token_to_id(('[SOS]'))], dtype = torch.int64)\n        self.eos_token = torch.tensor([tokenizer_src.token_to_id(('[EOS]'))], dtype = torch.int64)\n        self.pad_token = torch.tensor([tokenizer_src.token_to_id(('[PAD]'))], dtype = torch.int64)\n        \n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, index : Any) -> Any:\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n        \n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n        \n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0 :\n            raise ValueError('Sentence is too long')\n        \n        encoder_input = torch.cat(\n        [\n            self.sos_token, \n            torch.tensor(enc_input_tokens, dtype = torch.int64),\n            self.eos_token,\n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64)\n        ])\n        \n        decoder_input = torch.cat(\n        [\n            self.sos_token,\n            torch.tensor(dec_input_tokens, dtype = torch.int64),\n            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n        ])\n        \n        label = torch.cat(\n        [\n            torch.tensor(dec_input_tokens, dtype = torch.int64),\n            self.eos_token,\n            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n        ])\n        \n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            \"encoder_input\" : encoder_input,\n            \"decoder_input\" : decoder_input,\n            \"encoder_mask\"  : (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            \"decoder_mask\"  : (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n            \"label\"         : label,\n            \"src_text\"      : src_text,\n            \"tgt_text\"      : tgt_text\n            \n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(ds_train))  \nprint(type(train_ds))\nprint(type(val_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = BilingualDataset(train_ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\nval_ds = BilingualDataset(val_ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Size of ds_train: {len(ds_train)}\")\nprint(f\"Size of train_ds: {len(train_ds)}\")\nprint(f\"Size of val_ds: {len(val_ds)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len_src = 0\n\nmax_len_tgt = 0\n\nfor item in ds_train:\n    src_ids = tokenizer_src.encode(item ['translation'][config['lang_src']]).ids\n    tgt_ids = tokenizer_tgt.encode(item ['translation'][config['lang_tgt']]).ids\n    \n    max_len_src = max(max_len_src, len(src_ids))\n    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n    \nprint(f'Max length of source sentence: {max_len_src}')\nprint(f'Max length of target sentence: {max_len_tgt}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DataLoaders**","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True)\nval_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(train_dataloader))  \nprint(type(val_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Size of train_dataloader: {len(train_dataloader)}\")\nprint(f\"Size of val_dataloader: {len(val_dataloader)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'],config['seq_len'], config['d_model'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorboard\nfrom tqdm import tqdm\nimport warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training Loop** ","metadata":{}},{"cell_type":"code","source":"def train_model(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device {device}')\n    \n    Path(config['model_folder']).mkdir(parents=True, exist_ok = True)\n    \n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    \n    writer = SummaryWriter(config['experiment_name'])\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'], eps = 1e-9)\n    \n    initial_epoch = 0\n    global_step = 0\n    \n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n        \n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n    \n    for epoch in range(initial_epoch, config['num_epochs']):\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch :02d}')\n        for batch in batch_iterator:\n            \n            encoder_input = batch['encoder_input'].to(device) \n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n            \n            encoder_output = model.encode(encoder_input, encoder_mask) #(batch, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) #(batch, seq_len, d_model)\n            proj_output = model.project(decoder_output) #(batch, seq_len, tgt_vocab_size)\n            \n            label = batch['label'].to(device) #(batch, seq_len)\n            \n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)) #(batch, seq_len, tgt_vocab_size) -> (batch * seq_len, tgt_vocab_size)\n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n            \n            writer.add_scaler('train loss', loss.item(), global_step)\n            writer.flush()\n            \n            loss.backward()\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            global_step +=1\n            \n            model_filename = get_weights_file_path(config, f'{epoch:02d}')\n            torch.save({\n                'epoch' : epoch,\n                'model_state_dict' : model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'global_step' : global_step,\n            }, model_filename)\n    \nif __name__ == '__main__':\n    warnings.filterwarnings('ignore')\n    config = get_config()\n    train_model(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}